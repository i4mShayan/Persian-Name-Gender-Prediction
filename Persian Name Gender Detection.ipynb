{"cells":[{"cell_type":"markdown","metadata":{"id":"IZUvtwjBzjFG"},"source":["# Initialization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A1zTE_uX0LXx"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"NSOuaN8tznSh"},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cGnGQXv4MTFP"},"outputs":[],"source":["import pandas as pd\n","import os\n","import logging\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import json\n","import re\n","from tqdm import tqdm\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import accuracy_score, classification_report\n","from keras.preprocessing.sequence import pad_sequences\n","\n","from sklearn.preprocessing import OneHotEncoder\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing import sequence\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Embedding, Dense, Activation, Dropout, LSTM, Bidirectional\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.utils import plot_model\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n","\n","import numpy as np\n","import keras\n","from keras.models import Sequential\n","from keras.layers import LSTM, Dense\n","from keras.utils import to_categorical\n","from keras.metrics import Precision, Recall, CategoricalAccuracy\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve\n","import pickle\n","\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from keras.utils import to_categorical\n","import torch\n","from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","from transformers import EvalPrediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RgRd4yBSJUi-"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"3rfpT2A01oil"},"source":["# Constants"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d4G88pI_1rmr"},"outputs":[],"source":["DIR = ''\n","\n","DATA_DIR = DIR + 'data/'\n","RAW_DATA = DATA_DIR + 'FirstNames.xlsx'\n","CHAR_MAP_DATA = DATA_DIR + 'char_data.json'\n","ROBUST_DATA = DATA_DIR + 'robust_data.csv'\n","\n","LOG_DIR = DIR + 'logs/'\n","\n","LSTM_BINARY_LOG_DIR = LOG_DIR + 'lstm_binary/'\n","LSTM_MULTICLASS_LOG_DIR = LOG_DIR + 'lstm_multiclass/'\n","LSTM_MULTICLASS_FREQ_LOG_DIR = LOG_DIR + 'lstm_multiclass_freq/'\n","\n","FABERT_BINARY_DNN_LOG_DIR = LOG_DIR + 'fabert_dnn_binary/'\n","FABERT_MULTICLASS_DNN_LOG_DIR = LOG_DIR + 'fabert_dnn_multiclass/'"]},{"cell_type":"markdown","metadata":{"id":"G8vEwo_r2qup"},"source":["# Load Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T_zSCctg2aUh"},"outputs":[],"source":["df = pd.read_excel(RAW_DATA)\n","df"]},{"cell_type":"markdown","metadata":{"id":"pm9QFTqDRyrJ"},"source":["# Exploratory Data Analysis (EDA)"]},{"cell_type":"markdown","metadata":{"id":"lrYpkgAC5r0a"},"source":["## Pre-Processing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1B_8h8jl3DqQ"},"outputs":[],"source":["def unique_values(df):\n","  unique_values = df.apply(lambda x: x.unique())\n","  print(unique_values)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RYofXOLt5h07"},"outputs":[],"source":["unique_values(df)"]},{"cell_type":"markdown","metadata":{"id":"9eOxOKkMBQc0"},"source":["### Refining"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ld3Kkrk3A0_J"},"outputs":[],"source":["df.rename(\n","    columns={\n","        'Naam': 'Name',\n","        'Pesar': 'isMale',\n","        'Dokhtar': 'isFemale'\n","    },\n","    inplace=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IRh0TBh4AhNS"},"outputs":[],"source":["df['Rate'].replace({\n","    'بسيار نادر': 'Very Rare',\n","    'معمولي': 'Common',\n","    'پركاربرد': 'Frequently Used'\n","}, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rdXNN4vNIQh4"},"outputs":[],"source":["def determine_gender(row):\n","    if row['isFemale'] == 1 and row['isMale'] == 1:\n","        return 'Neutral'\n","    elif row['isFemale'] == 1:\n","        return 'Female'\n","    elif row['isMale'] == 1:\n","        return 'Male'\n","    else:\n","        return 'Missing'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cWlHz4JZ4Tto"},"outputs":[],"source":["df.fillna(0, inplace=True)\n","df['Gender'] = df.apply(determine_gender, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cOAolD_nAkuu"},"outputs":[],"source":["df.drop(columns=['isMale', 'isFemale'], inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fVQPQTiy5kQM"},"outputs":[],"source":["unique_values(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l5gPPeFTBNDr"},"outputs":[],"source":["df"]},{"cell_type":"markdown","metadata":{"id":"7tDSgG6WEECT"},"source":["### Text Refinement"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n6mp2TeZ87P9"},"outputs":[],"source":["# Get the unique characters in the names\n","vocab_chars = sorted(list(set(''.join(df['Name']))))\n","len(vocab_chars)"]},{"cell_type":"markdown","metadata":{"id":"IA-MaELqHODR"},"source":["#### Char Mapping Table"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Fk-yeaw8u_j"},"outputs":[],"source":["# with open(CHAR_MAP_DATA, 'w', encoding='utf-8') as f:\n","#     json.dump({'valid_chars': valid_chars, 'char_mappings': char_mappings}, f, ensure_ascii=False, indent=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BL1ACZocC03G"},"outputs":[],"source":["with open(CHAR_MAP_DATA, 'r', encoding='utf-8') as f:\n","    data = json.load(f)\n","\n","valid_chars = data['valid_chars']\n","char_mappings = data['char_mappings']\n","\n","translation_table = dict((ord(a), b) for a, b in char_mappings.items())"]},{"cell_type":"markdown","metadata":{"id":"qzHL-MK-HTEo"},"source":["#### Handling Whitespaces"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OQHe5x-R2YQD"},"outputs":[],"source":["def remove_zero_width_characters(text: str) -> str:\n","    text = text.replace(\"\\u200c\", \"\")\n","    text = text.replace(\"\\u200b\", \"\")\n","    text = text.replace(\"\\ufe0f\", \"\")\n","    text = text.replace(\"\\ufeff\", \"\")\n","    return text\n","\n","def replace_consecutive_whitespace(text: str) -> str:\n","    return re.sub(r\"\\s+\", \" \", text)"]},{"cell_type":"markdown","metadata":{"id":"8Adms43oHbe0"},"source":["#### Refinement Pipline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XcQkqmRNGqYV"},"outputs":[],"source":["def refine_name(text: str) -> str:\n","    if pd.isna(text):\n","        return text\n","\n","    text = remove_zero_width_characters(text)\n","    text = text.translate(translation_table)\n","    text = replace_consecutive_whitespace(text)\n","    return text.strip()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IFenpxHiHgdY"},"outputs":[],"source":["df['Name'] = df['Name'].apply(refine_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"28vtVwucEo8Q"},"outputs":[],"source":["translated_chars = sorted(list(set(''.join(df['Name']))))\n","len(translated_chars)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IwN0krkZOx-d"},"outputs":[],"source":["print(translated_chars)"]},{"cell_type":"markdown","metadata":{"id":"mg3ic4F4RW09"},"source":["### Duplicate Data"]},{"cell_type":"markdown","metadata":{"id":"CQFWqPxja19Q"},"source":["#### Exploring Duplicates"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OBO4gyxgQ0Ap"},"outputs":[],"source":["# Duplicate names with the same attributes\n","duplicates_same_attributes = df[df.duplicated(subset=['Name', 'Rate', 'Gender'], keep=False)]\n","duplicates_same_attributes.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vt-QQeM2XWoA"},"outputs":[],"source":["# Duplicates with different Gender but same Name and Rate\n","diff_gender = df.groupby(['Name', 'Rate']).filter(lambda x: x['Gender'].nunique() > 1)\n","diff_gender.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WQ5MBf5aYIzZ"},"outputs":[],"source":["# Duplicates with different Rate but same Name and Gender\n","diff_rate = df.groupby(['Name', 'Gender']).filter(lambda x: x['Rate'].nunique() > 1)\n","diff_rate.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FYWInu0XaLaJ"},"outputs":[],"source":["# Duplicates where only the Name is the same, but both Rate and Gender are different\n","diff_name_only = df.groupby('Name').filter(\n","    lambda x: (x['Rate'].nunique() > 1) & (x['Gender'].nunique() > 1)\n",")\n","diff_name_only.head()"]},{"cell_type":"markdown","metadata":{"id":"fkFposUTa5Uw"},"source":[]},{"cell_type":"markdown","metadata":{"id":"0Mq7xcABa-bz"},"source":["#### Handling Duplicates"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9jxxZMJYbAwV"},"outputs":[],"source":["rate_priority = {\n","    'Frequently Used': 3,\n","    'Common': 2,\n","    'Very Rare': 1\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eQ8wcQdwdH7i"},"outputs":[],"source":["df['Rate_Priority'] = df['Rate'].map(rate_priority)\n","\n","df_sorted = df.sort_values(by=['Name', 'Rate_Priority'], ascending=[True, False])\n","\n","# If there's multiple values for gender, then select 'Neutral'\n","def select_gender(group):\n","  if group['Gender'].nunique() > 1:\n","    group['Gender'] = 'Neutral'\n","  return group\n","\n","df_final = df_sorted.groupby('Name').apply(select_gender)\n","\n","# Drop duplicates, keeping the first (which is the highest priority due to sorting)\n","df_final_unique = df_final.drop_duplicates(subset='Name', keep='first')\n","\n","df_final_unique = df_final_unique.drop(columns=['Rate_Priority'])\n","df_final_unique.reset_index(drop=True, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P6cBxc0pF2OG"},"outputs":[],"source":["df[df['Name']=='عرفان']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hgWvNmCDeup1"},"outputs":[],"source":["df_final_unique[df_final_unique['Name']=='عرفان']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PW89APH2feZB"},"outputs":[],"source":["print(df.shape)\n","print(df_final_unique.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Gqj7ua2h7ys"},"outputs":[],"source":["df = df_final_unique"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S9gfkxqhiq_0"},"outputs":[],"source":["df.to_csv(ROBUST_DATA, index=False)"]},{"cell_type":"markdown","metadata":{"id":"qYXvt0N2IF2C"},"source":["## Distributions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"msM0an7-J9fg"},"outputs":[],"source":["def plot_distribution(df: pd.DataFrame, column: str, palette='viridis', title=None, xlabel=None, ylabel='Count'):\n","    plt.figure(figsize=(8, 6))\n","    ax = sns.countplot(data=df, x=column, palette=palette, hue=column, legend=False)\n","\n","    for p in ax.patches:\n","        height = p.get_height()\n","        ax.text(p.get_x() + p.get_width() / 2., height + 0.1, int(height), ha=\"center\", va=\"bottom\")\n","\n","    if title:\n","        plt.title(title)\n","    if xlabel:\n","        plt.xlabel(xlabel)\n","    plt.ylabel(ylabel)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3JNK5SNxBEPE"},"outputs":[],"source":["plot_distribution(df, 'Gender', title='Gender Distribution', xlabel='Gender')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pRucIJBpKFB6"},"outputs":[],"source":["plot_distribution(df, 'Rate', title='Rate Distribution', xlabel='Rate')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ylbS-XAL2O5"},"outputs":[],"source":["def plot_combined_distribution(df: pd.DataFrame, x_column: str, hue_column: str, palette='viridis', title=None, xlabel=None, ylabel='Count'):\n","    plt.figure(figsize=(10, 6))\n","    ax = sns.countplot(data=df, x=x_column, hue=hue_column, palette=palette)\n","\n","    for p in ax.patches:\n","        height = p.get_height()\n","        ax.text(p.get_x() + p.get_width() / 2., height + 0.1, int(height), ha=\"center\", va=\"bottom\")\n","\n","    if title:\n","        plt.title(title)\n","    if xlabel:\n","        plt.xlabel(xlabel)\n","    plt.ylabel(ylabel)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QG12M9LML6Hg"},"outputs":[],"source":["plot_combined_distribution(df, 'Gender', 'Rate', title='Gender and Rate Distribution', xlabel='Gender')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gg2KvXUajwTT"},"outputs":[],"source":["df['NameLength'] = df['Name'].apply(len)\n","length_counts = df['NameLength'].value_counts().sort_index()\n","\n","plt.figure(figsize=(10, 6))\n","bars = plt.bar(length_counts.index, length_counts.values)\n","\n","for bar in bars:\n","    yval = bar.get_height()\n","    plt.text(bar.get_x() + bar.get_width()/2, yval, int(yval), ha='center', va='bottom')\n","\n","plt.xticks(length_counts.index)\n","plt.xlabel('Length of Name')\n","plt.ylabel('Frequency')\n","plt.title('Frequency of Name Lengths')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"wCP1Dnz6O64J"},"source":["# Load Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yx_OWPXQO8jL"},"outputs":[],"source":["df = pd.read_csv(ROBUST_DATA)\n","df"]},{"cell_type":"markdown","metadata":{"id":"IOexfIZUjdkT"},"source":["# Models"]},{"cell_type":"markdown","source":["## Load Model"],"metadata":{"id":"91anmp_cg38A"}},{"cell_type":"code","source":["from keras.models import load_model\n","\n","model = load_model(os.path.join(LSTM_BINARY_LOG_DIR, 'best_model.keras'), safe_mode=False)"],"metadata":{"id":"onhLLB5Gg3Ph"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8uyhPHhurtVR"},"source":["## 1. Character-level Bi-LSTM Models"]},{"cell_type":"code","source":["def encode_data(df, max_len=20):\n","    label_encoder = LabelEncoder()\n","    y = label_encoder.fit_transform(df['Gender'])\n","\n","    char_set = sorted(set(''.join(df['Name'])))\n","    char_to_int = {c: i + 1 for i, c in enumerate(char_set)}  # +1 for padding\n","\n","    X = np.array([[char_to_int.get(char, 0) for char in name] + [0] * (max_len - len(name)) for name in df['Name']])\n","\n","    return X, y, label_encoder, char_to_int"],"metadata":{"id":"vQdqzueyHkdM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def split_data(df, split_frequently_used=False):\n","    very_rare = df[df['Rate'] == 'Very Rare']\n","    common = df[df['Rate'] == 'Common']\n","    frequently_used = df[df['Rate'] == 'Frequently Used']\n","\n","    train_very_rare, test_very_rare = train_test_split(very_rare, test_size=0.2, random_state=42)\n","    train_common, test_common = train_test_split(common, test_size=0.2, random_state=42)\n","\n","    if split_frequently_used:\n","        train_frequently_used, test_frequently_used = train_test_split(frequently_used, test_size=0.2, random_state=42)\n","        return train_very_rare, test_very_rare, train_common, test_common, train_frequently_used, test_frequently_used\n","    else:\n","        return train_very_rare, test_very_rare, train_common, test_common, frequently_used"],"metadata":{"id":"ZqZ89ox5HlwX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class F1Score(tf.keras.metrics.Metric):\n","    def __init__(self, name='f1_score', **kwargs):\n","        super(F1Score, self).__init__(name=name, **kwargs)\n","        self.precision = Precision(name='precision')\n","        self.recall = Recall(name='recall')\n","\n","    def update_state(self, y_true, y_pred, sample_weight=None):\n","        self.precision.update_state(y_true, y_pred, sample_weight)\n","        self.recall.update_state(y_true, y_pred, sample_weight)\n","\n","    def result(self):\n","        precision = self.precision.result()\n","        recall = self.recall.result()\n","        return 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))\n","\n","    def reset_states(self):\n","        self.precision.reset_states()\n","        self.recall.reset_states()"],"metadata":{"id":"XvOmWG67HnLN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_model(input_dim, output_dim, max_len, is_binary=True):\n","    model = Sequential()\n","    model.add(Embedding(input_dim=input_dim, output_dim=32, input_length=max_len))\n","    model.add(Bidirectional(LSTM(64, return_sequences=True), backward_layer=LSTM(64, return_sequences=True, go_backwards=True)))\n","    model.add(Dropout(0.2))\n","    model.add(Bidirectional(LSTM(64)))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(32, activation='relu', activity_regularizer=l2(0.002)))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(8, activation='relu', activity_regularizer=l2(0.002)))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(1 if is_binary else output_dim, activation='sigmoid' if is_binary else 'softmax', activity_regularizer=l2(0.002)))\n","\n","    metrics = (['accuracy', Precision(name='precision'), Recall(name='recall'), F1Score()]\n","           if is_binary else\n","           [CategoricalAccuracy(name='accuracy'), Precision(name='precision'), Recall(name='recall'), F1Score()])\n","\n","    # lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n","    # initial_learning_rate=1e-2,\n","    # decay_steps=10000,\n","    # decay_rate=0.9)\n","    # opt = keras.optimizers.Adam(learning_rate=lr_schedule)\n","\n","    model.compile(\n","        loss='binary_crossentropy' if is_binary else 'categorical_crossentropy',\n","        optimizer='adam',\n","        # optimizer=opt,\n","        metrics=metrics\n","        # metrics=['accuracy', Precision(name='precision'), Recall(name='recall')] if is_binary else [CategoricalAccuracy(name='accuracy'), Precision(name='precision'), Recall(name='recall')]\n","    )\n","\n","    return model"],"metadata":{"id":"RzITQ830Ho0T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_model(model, X_train, y_train, X_test, y_test, model_path, epochs=50, batch_size=32):\n","    mc = ModelCheckpoint(model_path, monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n","    reduce_lr_acc = ReduceLROnPlateau(monitor='val_f1_score', factor=0.7, patience=10, verbose=1, min_delta=1e-4, mode='max')\n","\n","    callbacks = [mc, reduce_lr_acc]\n","\n","    return model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size, callbacks=callbacks)"],"metadata":{"id":"fZZjLLIxHq2r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_model_plot(model, filename='model_architecture.png'):\n","    plt_path = os.path.join(LOG_FOLDER, filename)\n","    plot_model(model, to_file=plt_path, show_shapes=True, show_layer_names=True)\n","    print(f\"Model architecture saved to {plt_path}\")"],"metadata":{"id":"rhVd-BAuHsMa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_metrics(history, metrics, title=None):\n","    plt.figure(figsize=(15, 10))\n","\n","    for i, metric in enumerate(metrics):\n","        plt.subplot(2, 2, i + 1)\n","\n","        metric_values = history.history.get(metric, [])\n","        val_metric_values = history.history.get(f'val_{metric}', [])\n","\n","        if metric_values:\n","            plt.plot(metric_values, label=f'Train {metric}')\n","\n","        if val_metric_values:\n","            plt.plot(val_metric_values, label=f'Val {metric}')\n","\n","        plt.title(f'{metric.capitalize()} Over Epochs')\n","        plt.xlabel('Epochs')\n","        plt.ylabel(metric.capitalize())\n","        plt.legend()\n","        plt.grid()\n","\n","    plt.tight_layout()\n","\n","    if title:\n","        plt.suptitle(title, fontsize=16)\n","        plt.subplots_adjust(top=0.9)\n","\n","    plt_path = os.path.join(LOG_FOLDER, 'training_metrics.png')\n","    plt.savefig(plt_path)\n","    plt.show()\n","\n","def predict_and_evaluate(model, new_name, char_to_int, max_len, label_encoder):\n","    new_X = np.array([[char_to_int.get(char, 0) for char in new_name] + [0] * (max_len - len(new_name))])\n","    prediction = model.predict(new_X)\n","    print(f\"Model Prediction: {prediction}\")\n","    if len(prediction[0]) > 1:  # Multi-class classification\n","        predicted_class_index = np.argmax(prediction, axis=1)\n","        predicted_gender = label_encoder.inverse_transform(predicted_class_index)[0]\n","    else:  # Binary classification\n","        if prediction > 0.65:\n","            predicted_gender = 'Male'\n","        elif prediction < 0.35:\n","            predicted_gender = 'Female'\n","        else:\n","            predicted_gender = 'Neutral'\n","\n","    print(f'The predicted gender for {new_name} is {predicted_gender}')\n","\n","def test_random_samples(data_split, split_name, char_to_int, model, label_encoder, max_len):\n","    print(f\"\\nTesting 5 random samples from {split_name}:\\n\")\n","    sample_data = data_split.sample(5)\n","\n","    for index, row in sample_data.iterrows():\n","        name = row['Name']\n","        actual_gender = row['Gender']\n","        encoded_name = np.array([[char_to_int.get(char, 0) for char in name] + [0] * (max_len - len(name))])\n","\n","        prediction = model.predict(encoded_name, verbose=0)\n","\n","        if len(prediction[0]) > 1:  # Multi-class classification\n","            predicted_class_index = np.argmax(prediction, axis=1)\n","            predicted_gender = label_encoder.inverse_transform(predicted_class_index)[0]\n","        else:  # Binary classification\n","            if prediction > 0.65:\n","                predicted_gender = 'Male'\n","            elif prediction < 0.35:\n","                predicted_gender = 'Female'\n","            else:\n","                predicted_gender = 'Neutral'\n","\n","        actual_gender_decoded = label_encoder.inverse_transform([label_encoder.transform([actual_gender])[0]])[0]\n","\n","        print(f\"Name: {name}, Actual: {actual_gender_decoded}, Predicted: {predicted_gender}\")\n","\n","def evaluate_and_plot_accuracy(data_splits, split_names, char_to_int, model, label_encoder, max_len):\n","    results = {}\n","\n","    for data_split, split_name in zip(data_splits, split_names):\n","        X_split = np.array([[char_to_int.get(char, 0) for char in name] + [0] * (max_len - len(name)) for name in data_split['Name']])\n","        y_true = label_encoder.transform(data_split['Gender'])\n","\n","        predictions = model.predict(X_split, verbose=0)\n","        predicted_class_indices = np.argmax(predictions, axis=1) if len(predictions[0]) > 1 else (predictions > 0.5).astype(int).flatten()\n","\n","        correct_predictions = (predicted_class_indices == y_true).sum()\n","        wrong_predictions = (predicted_class_indices != y_true).sum()\n","\n","        results[split_name] = {\n","            'correct': correct_predictions,\n","            'wrong': wrong_predictions,\n","            'accuracy': correct_predictions / (correct_predictions + wrong_predictions)\n","        }\n","\n","    plot_accuracy_results(results)\n","\n","def plot_accuracy_results(results):\n","    split_names = list(results.keys())\n","    correct_counts = [results[split]['correct'] for split in split_names]\n","    wrong_counts = [results[split]['wrong'] for split in split_names]\n","    accuracies = [results[split]['accuracy'] for split in split_names]\n","\n","    x = np.arange(len(split_names))\n","    width = 0.35\n","\n","    fig, ax = plt.subplots(figsize=(12, 7))\n","\n","    bars_correct = ax.bar(x - width/2, correct_counts, width, label='Correct', color='g')\n","    bars_wrong = ax.bar(x + width/2, wrong_counts, width, label='Wrong', color='r')\n","\n","    for i, accuracy in enumerate(accuracies):\n","        ax.text(x[i], max(correct_counts[i], wrong_counts[i]) + 0.5, f'Acc: {accuracy:.2f}', ha='center', fontsize=12)\n","\n","    ax.set_xlabel('Rate Category')\n","    ax.set_ylabel('Number of Predictions')\n","    ax.set_title('Correct and Wrong Predictions by Rate')\n","    ax.set_xticks(x)\n","    ax.set_xticklabels(split_names)\n","    ax.legend()\n","\n","    plt.tight_layout()\n","    plt_path = os.path.join(LOG_FOLDER, 'accuracy_results.png')\n","    plt.savefig(plt_path)\n","    plt.show()\n","\n","def evaluate_confusion_matrix(data_splits, split_names, char_to_int, model, label_encoder, max_len):\n","    all_y_true = []\n","    all_y_pred = []\n","\n","    for data_split, split_name in zip(data_splits, split_names):\n","        X_split = np.array([[char_to_int.get(char, 0) for char in name] + [0] * (max_len - len(name)) for name in data_split['Name']])\n","        y_true = label_encoder.transform(data_split['Gender'])\n","\n","        predictions = model.predict(X_split, verbose=0)\n","        predicted_class_indices = np.argmax(predictions, axis=1) if len(predictions[0]) > 1 else (predictions > 0.5).astype(int).flatten()\n","\n","        all_y_true.extend(y_true)\n","        all_y_pred.extend(predicted_class_indices)\n","\n","    conf_matrix = confusion_matrix(all_y_true, all_y_pred)\n","    conf_matrix_df = pd.DataFrame(conf_matrix, index=label_encoder.classes_, columns=label_encoder.classes_)\n","\n","    plot_confusion_matrix(conf_matrix_df)\n","\n","def plot_confusion_matrix(conf_matrix_df):\n","    plt.figure(figsize=(10, 7))\n","    sns.heatmap(conf_matrix_df, annot=True, cmap='Blues', fmt='g')\n","    plt.title('Confusion Matrix')\n","    plt.xlabel('Predicted')\n","    plt.ylabel('Actual')\n","    plt_path = os.path.join(LOG_FOLDER, 'confusion_matrix.png')\n","    plt.savefig(plt_path)\n","    plt.show()"],"metadata":{"id":"omeh-lsIHkIT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### a. Binary Classification"],"metadata":{"id":"TgBoRrMpHh1z"}},{"cell_type":"markdown","source":["#### Train"],"metadata":{"id":"xLYlbmaopSAJ"}},{"cell_type":"code","source":["LOG_FOLDER = LSTM_BINARY_LOG_DIR\n","\n","os.makedirs(LOG_FOLDER, exist_ok=True)"],"metadata":{"id":"cf7tAl5ouXwy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = df[df['Gender'] != 'Neutral']\n","\n","max_len = 20\n","X, y, label_encoder, char_to_int = encode_data(df, max_len)\n","is_binary = len(np.unique(y)) == 2\n","\n","# Data Splitting\n","train_very_rare, test_very_rare, train_common, test_common, train_frequently_used, test_frequently_used = split_data(df, split_frequently_used=True)\n","\n","# Combine Train and Test Sets\n","train_data = pd.concat([train_very_rare, train_common, train_frequently_used])\n","test_data = pd.concat([test_very_rare, test_common, test_frequently_used])\n","\n","# Prepare Training and Testing Data\n","X_train = np.array([[char_to_int[char] for char in name] + [0] * (max_len - len(name)) for name in train_data['Name']])\n","y_train = to_categorical(label_encoder.transform(train_data['Gender'])) if not is_binary else label_encoder.transform(train_data['Gender'])\n","X_test = np.array([[char_to_int[char] for char in name] + [0] * (max_len - len(name)) for name in test_data['Name']])\n","y_test = to_categorical(label_encoder.transform(test_data['Gender'])) if not is_binary else label_encoder.transform(test_data['Gender'])\n","\n","# Build and Train Model\n","input_dim = len(char_to_int) + 1\n","output_dim = len(label_encoder.classes_)\n","model = build_model(input_dim, output_dim, max_len, is_binary)\n","history = train_model(model, X_train, y_train, X_test, y_test, model_path=LOG_FOLDER + 'best_model.keras', epochs=100, batch_size=150)\n","\n","with open(os.path.join(LOG_FOLDER, 'model.history'), 'wb') as file_pi:\n","    pickle.dump(history.history, file_pi)"],"metadata":{"id":"PECm5gcTZK-b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["save_model_plot(model)"],"metadata":{"id":"MasUFnNs1hlz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Evaluation"],"metadata":{"id":"PUw5dW6upUNX"}},{"cell_type":"code","source":["plot_metrics(history, ['accuracy', 'precision', 'recall', 'f1_score'], title='Training and Validation Metrics')"],"metadata":{"id":"cajA7r79fVVm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_splits = [train_very_rare, test_very_rare, train_common, test_common, train_frequently_used, test_frequently_used]\n","split_names = [\"Train Very Rare\", \"Test Very Rare\", \"Train Common\", \"Test Common\", \"Train Frequently Used\", \"Test Frequently Used\"]"],"metadata":{"id":"F4ZsG_nEfanJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["evaluate_and_plot_accuracy(data_splits, split_names, char_to_int, model, label_encoder, max_len)"],"metadata":{"id":"oV0KaemMgeGL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["evaluate_confusion_matrix(data_splits, split_names, char_to_int, model, label_encoder, max_len)"],"metadata":{"id":"hLO_wTpxupky"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Test"],"metadata":{"id":"iAGAoEYLpXhE"}},{"cell_type":"code","source":["test_random_samples(test_common, \"Train Common\", char_to_int, model, label_encoder, max_len)"],"metadata":{"id":"aS4UtjNXgP3g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predict_and_evaluate(model, 'آرمین', char_to_int, max_len, label_encoder)"],"metadata":{"id":"s7CJ4XQniDRh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### b. Multi-Class Classification\n"],"metadata":{"id":"kLhMYS2OM1mO"}},{"cell_type":"markdown","source":["#### Train"],"metadata":{"id":"6wti6XXln829"}},{"cell_type":"code","source":["LOG_FOLDER = LSTM_MULTICLASS_LOG_DIR\n","\n","os.makedirs(LOG_FOLDER, exist_ok=True)"],"metadata":{"id":"LodJVewKutWs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_len = 20\n","X, y, label_encoder, char_to_int = encode_data(df, max_len)\n","is_binary = len(np.unique(y)) == 2\n","\n","# Data Splitting\n","train_very_rare, test_very_rare, train_common, test_common, train_frequently_used, test_frequently_used = split_data(df, split_frequently_used=True)\n","\n","# Combine Train and Test Sets\n","train_data = pd.concat([train_very_rare, train_common, train_frequently_used])\n","test_data = pd.concat([test_very_rare, test_common, test_frequently_used])\n","\n","# Prepare Training and Testing Data\n","X_train = np.array([[char_to_int[char] for char in name] + [0] * (max_len - len(name)) for name in train_data['Name']])\n","y_train = to_categorical(label_encoder.transform(train_data['Gender'])) if not is_binary else label_encoder.transform(train_data['Gender'])\n","X_test = np.array([[char_to_int[char] for char in name] + [0] * (max_len - len(name)) for name in test_data['Name']])\n","y_test = to_categorical(label_encoder.transform(test_data['Gender'])) if not is_binary else label_encoder.transform(test_data['Gender'])\n","\n","# Build and Train Model\n","input_dim = len(char_to_int) + 1\n","output_dim = len(label_encoder.classes_)\n","model = build_model(input_dim, output_dim, max_len, is_binary)\n","history = train_model(model, X_train, y_train, X_test, y_test, model_path=LOG_FOLDER + 'best_model.keras', epochs=100, batch_size=100)\n","\n","with open(os.path.join(LOG_FOLDER, 'model.history'), 'wb') as file_pi:\n","    pickle.dump(history.history, file_pi)"],"metadata":{"id":"9hmW3S9rRuAX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["save_model_plot(model)"],"metadata":{"id":"rxj1Xk6YIWAM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Evaluation"],"metadata":{"id":"AYux0K28ot7o"}},{"cell_type":"code","source":["plot_metrics(history, ['accuracy', 'precision', 'recall', 'f1_score'], title='Training and Validation Metrics')"],"metadata":{"id":"Rt0v-UbgR4Et"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_splits = [train_very_rare, test_very_rare, train_common, test_common, train_frequently_used, test_frequently_used]\n","split_names = [\"Train Very Rare\", \"Test Very Rare\", \"Train Common\", \"Test Common\", \"Train Frequently Used\", \"Test Frequently Used\"]"],"metadata":{"id":"8pWdjcOUR6Tv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["evaluate_and_plot_accuracy(data_splits, split_names, char_to_int, model, label_encoder, max_len)"],"metadata":{"id":"nWUVZa7DR8CC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["evaluate_confusion_matrix(data_splits, split_names, char_to_int, model, label_encoder, max_len)"],"metadata":{"id":"v3IucczER-Ew"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Test"],"metadata":{"id":"Ou-DO52TozYg"}},{"cell_type":"code","source":["test_random_samples(test_common, \"Test Common\", char_to_int, model, label_encoder, max_len)"],"metadata":{"id":"kwRN_QuAoDRH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predict_and_evaluate(model, 'رضا', char_to_int, max_len, label_encoder)"],"metadata":{"id":"LO6idNMpoEZa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Hazm Based Model"],"metadata":{"id":"Fegg8wWhDVvl"}},{"cell_type":"markdown","source":["### Installations"],"metadata":{"id":"CAwItQmWJzxa"}},{"cell_type":"code","source":["!sudo apt-get install -y asciidoc megatools"],"metadata":{"id":"51JWdDWGAuyJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd '/content/drive/MyDrive/Work/Utils'"],"metadata":{"id":"fyNcqqPnAzhN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!curl 'https://megatools.megous.com/builds/megatools-1.10.3.tar.gz' | tar xz"],"metadata":{"id":"tP4NTqvLI5A-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Work/Utils/megatools-1.10.3/\n","!./configure; make; make install"],"metadata":{"id":"gZWE8cOmBvza"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!megadl --path '/content/drive/MyDrive/Work/Utils' 'https://mega.nz/file/GqZUlbpS#XRYP5FHbPK2LnLZ8IExrhrw3ZQ-jclNSVCz59uEhrxY'"],"metadata":{"id":"kdXyTHf0B1Ii"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd '/content/drive/MyDrive/Work/Utils'\n","!unzip fasttext_model.zip -d '/content/drive/MyDrive/Work/Utils'"],"metadata":{"id":"aGTeKLAbB7Tk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls"],"metadata":{"id":"GWX_En-BKle_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## FaBERT Based Model"],"metadata":{"id":"Z7YUtoz7J5j3"}},{"cell_type":"markdown","source":["#### ML Models"],"metadata":{"id":"_mjbcS7H1LAj"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForMaskedLM\n","import torch\n","\n","# Load tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(\"sbunlp/fabert\")\n","model = AutoModelForMaskedLM.from_pretrained(\"sbunlp/fabert\")\n","\n","# Name of the person\n","name = \"علی\"\n","\n","# Tokenize the name\n","tokens = tokenizer(name, return_tensors='pt')\n","\n","print(tokens)\n","\n","# Get BERT embeddings\n","with torch.no_grad():\n","    outputs = model(**tokens, output_hidden_states=True)\n","\n","# Extract the hidden states from the outputs\n","hidden_states = outputs.hidden_states\n","\n","# The last hidden state (the last layer's embeddings)\n","embeddings = hidden_states[-1]\n","\n","# Average the embeddings of all tokens to get a single vector for the name\n","name_embedding = embeddings.mean(dim=1).squeeze()\n","\n","print(name_embedding)"],"metadata":{"id":"ThQPV48-PuhM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(name_embedding))"],"metadata":{"id":"il3EGvbQP-ss"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Filter out neutral gender entries and select a subset of names\n","df = df[df['Gender'] != 'Neutral']\n","names = df['Name'].tolist()\n","genders = df['Gender'].tolist()\n","\n","# Function to generate embeddings\n","def get_name_embedding(name):\n","    tokens = tokenizer(name, return_tensors='pt')\n","    with torch.no_grad():\n","        outputs = model(**tokens, output_hidden_states=True)\n","    hidden_states = outputs.hidden_states\n","    embeddings = hidden_states[-1]\n","    name_embedding = embeddings.mean(dim=1).squeeze().numpy()\n","    return name_embedding\n","\n","# Use tqdm to show progress while generating embeddings\n","embeddings = []\n","for name in tqdm(names, desc=\"Generating embeddings\"):\n","    embedding = get_name_embedding(name)\n","    embeddings.append(embedding)\n","\n","# Convert gender labels to numeric\n","label_mapping = {'Male': 0, 'Female': 1}  # Adjust according to your dataset\n","genders = [label_mapping[gender] for gender in genders]\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(embeddings, genders, test_size=0.2, random_state=42)\n","\n","# Train a Logistic Regression classifier\n","clf = LogisticRegression(max_iter=1000)\n","clf.fit(X_train, y_train)\n","\n","y_pred = clf.predict(X_test)\n","\n","accuracy = accuracy_score(y_test, y_pred)\n","report = classification_report(y_test, y_pred)\n","\n","print(f\"\\nAccuracy: {accuracy:.4f}\")\n","print(\"Classification Report:\")\n","print(report)"],"metadata":{"id":"EeHwiFBqRqdF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict_gender(name):\n","    name_embedding = get_name_embedding(name)\n","    gender_prediction = clf.predict([name_embedding])[0]\n","    reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n","    return reverse_label_mapping.get(gender_prediction, \"Unknown\")\n","\n","input_name = \"ایمانه\"\n","predicted_gender = predict_gender(input_name)\n","print(f\"The predicted gender for the name '{input_name}' is: {predicted_gender}\")"],"metadata":{"id":"Upx2JHK-TnCz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModelForMaskedLM\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","from tqdm import tqdm"],"metadata":{"id":"kDdPcWZLYE5Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(embeddings, genders, test_size=0.2, random_state=42)\n","\n","# Initialize classifiers\n","classifiers = {\n","    'Logistic Regression': LogisticRegression(max_iter=1000),\n","    'SVM': SVC(),\n","    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n","    'Extra Trees': ExtraTreesClassifier(n_estimators=100, random_state=42)\n","}\n","\n","for name, clf in classifiers.items():\n","    print(f\"\\nTraining {name}...\")\n","    clf.fit(X_train, y_train)\n","\n","    y_pred = clf.predict(X_test)\n","\n","    accuracy = accuracy_score(y_test, y_pred)\n","    report = classification_report(y_test, y_pred)\n","\n","    print(f\"{name} Accuracy: {accuracy:.4f}\")\n","    print(f\"{name} Classification Report:\")\n","    print(report)"],"metadata":{"id":"V_yNr3zvYFqu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.ensemble import VotingClassifier\n","\n","classifiers = {\n","    'Logistic Regression': LogisticRegression(max_iter=1000),\n","    'SVM': SVC(probability=True),\n","    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n","    'Extra Trees': ExtraTreesClassifier(n_estimators=100, random_state=42)\n","}\n","\n","voting_clf = VotingClassifier(estimators=[(name, clf) for name, clf in classifiers.items()], voting='soft')\n","voting_clf.fit(X_train, y_train)\n","\n","y_pred = voting_clf.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","report = classification_report(y_test, y_pred)\n","\n","print(f\"Voting Classifier Accuracy: {accuracy:.4f}\")\n","print(\"Voting Classifier Classification Report:\")\n","print(report)\n"],"metadata":{"id":"iE511kt0Yoww"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### DNN Model"],"metadata":{"id":"qyDxyiXp1QlN"}},{"cell_type":"code","source":["def encode_data(df, max_len=20):\n","    label_encoder = LabelEncoder()\n","    y = label_encoder.fit_transform(df['Gender'])\n","\n","    char_set = sorted(set(''.join(df['Name'])))\n","    char_to_int = {c: i + 1 for i, c in enumerate(char_set)}  # +1 for padding\n","\n","    X = np.array([[char_to_int.get(char, 0) for char in name] + [0] * (max_len - len(name)) for name in df['Name']])\n","\n","    return X, y, label_encoder, char_to_int\n","\n","def build_bert_model(num_labels=2):\n","    model = AutoModelForSequenceClassification.from_pretrained(\"sbunlp/fabert\", num_labels=num_labels)\n","    return model\n","\n","def compute_metrics(eval_pred: EvalPrediction):\n","    predictions, labels = eval_pred\n","    preds = np.argmax(predictions, axis=1)  # Get the predicted class labels\n","    accuracy = accuracy_score(labels, preds)\n","    precision = precision_score(labels, preds, average='weighted')\n","    recall = recall_score(labels, preds, average='weighted')\n","    f1 = f1_score(labels, preds, average='weighted')\n","    return {\n","        'accuracy': accuracy,\n","        'precision': precision,\n","        'recall': recall,\n","        'f1': f1\n","    }\n","\n","def train_model_with_train_metrics(\n","    model,\n","    train_dataset,\n","    val_dataset,\n","    tokenizer,\n","    model_path,\n","    log_folder,\n","    epochs=100,\n","    batch_size=100,\n","):\n","    os.makedirs(log_folder, exist_ok=True)\n","\n","    training_args = TrainingArguments(\n","        output_dir=log_folder,\n","        num_train_epochs=epochs,\n","        per_device_train_batch_size=batch_size,\n","        per_device_eval_batch_size=batch_size,\n","        evaluation_strategy=\"epoch\",\n","        logging_dir=os.path.join(log_folder, \"logs\"),\n","        logging_steps=1,\n","        save_total_limit=1,\n","        load_best_model_at_end=False,\n","        save_strategy=\"no\",\n","    )\n","\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=val_dataset,\n","        compute_metrics=compute_metrics,\n","    )\n","\n","    trainer.train()\n","\n","    train_outputs = trainer.predict(train_dataset)\n","    train_accuracy, train_precision, train_recall, train_f1 = compute_metrics(\n","        EvalPrediction(predictions=train_outputs.predictions, label_ids=train_outputs.label_ids)\n","    ).values()\n","\n","    train_metrics_history = [\n","        {\n","            \"epoch\": epoch + 1,\n","            \"train_accuracy\": train_accuracy,\n","            \"train_precision\": train_precision,\n","            \"train_recall\": train_recall,\n","            \"train_f1\": train_f1,\n","        }\n","        for epoch in range(epochs)\n","    ]\n","\n","    model.save_pretrained(model_path)\n","    tokenizer.save_pretrained(model_path)\n","\n","    with open(os.path.join(log_folder, \"model.history\"), \"wb\") as file_pi:\n","        pickle.dump(trainer.state.log_history, file_pi)\n","\n","    print(f\"Training history saved in {log_folder}\")\n","\n","    return trainer, trainer.state.log_history, train_metrics_history"],"metadata":{"id":"FroctUYs1WVk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["LOG_FOLDER = FABERT_BINARY_DNN_LOG_DIR\n","\n","os.makedirs(LOG_FOLDER, exist_ok=True)\n","\n","df = df[df['Gender'] != 'Neutral']"],"metadata":{"id":"5_amTZsF34br"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_len = 20\n","X, y, label_encoder, char_to_int = encode_data(df, max_len)\n","is_binary = len(np.unique(y)) == 2\n","\n","train_very_rare, test_very_rare, train_common, test_common, train_frequently_used, test_frequently_used = split_data(df, split_frequently_used=True)\n","\n","# Combine Train and Test Sets\n","train_data = pd.concat([train_very_rare, train_common, train_frequently_used])\n","test_data = pd.concat([test_very_rare, test_common, test_frequently_used])\n","\n","# Load the tokenizer and BERT model\n","tokenizer = AutoTokenizer.from_pretrained(\"sbunlp/fabert\")\n","model = AutoModelForSequenceClassification.from_pretrained(\"sbunlp/fabert\", num_labels=2 if is_binary else len(label_encoder.classes_))\n","\n","# Prepare training and testing data using BERT tokenizer\n","train_encodings = tokenizer(train_data['Name'].tolist(), truncation=True, padding=True, max_length=max_len, return_tensors='pt')\n","test_encodings = tokenizer(test_data['Name'].tolist(), truncation=True, padding=True, max_length=max_len, return_tensors='pt')\n","\n","X_train = train_encodings['input_ids']\n","attention_mask_train = train_encodings['attention_mask']\n","y_train = torch.tensor(label_encoder.transform(train_data['Gender']))\n","\n","X_test = test_encodings['input_ids']\n","attention_mask_test = test_encodings['attention_mask']\n","y_test = torch.tensor(label_encoder.transform(test_data['Gender']))"],"metadata":{"id":"GFhqKa9V1jiL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class GenderDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","# Create the datasets\n","train_dataset = GenderDataset(train_encodings, y_train)\n","val_dataset = GenderDataset(test_encodings, y_test)"],"metadata":{"id":"-ucvRONo8MJY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer, history = train_model_with_train_metrics(\n","    model=model,\n","    train_dataset=train_dataset,\n","    val_dataset=val_dataset,\n","    tokenizer=tokenizer,\n","    model_path=os.path.join(LOG_FOLDER, 'best_model'),\n","    log_folder=LOG_FOLDER,\n","    epochs=5,\n","    batch_size=256\n",")"],"metadata":{"id":"tKmeZyFV4HZm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict_and_evaluate(model, tokenizer, new_name, label_encoder, max_len=128):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","\n","    encodings = tokenizer([new_name], truncation=True, padding=True, max_length=max_len, return_tensors='pt')\n","    new_X = encodings['input_ids'].to(device)\n","    attention_mask = encodings['attention_mask'].to(device)\n","\n","    with torch.no_grad():\n","        prediction = model(new_X, attention_mask=attention_mask).logits\n","    predicted_class_index = torch.argmax(prediction, dim=1).item()\n","\n","    predicted_gender = label_encoder.inverse_transform([predicted_class_index])[0]\n","    print(f'The predicted gender for {new_name} is {predicted_gender}')"],"metadata":{"id":"BgTRYr2YDGQp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_metrics(history, train_metrics_history):\n","    train_loss = []\n","    eval_loss = []\n","    eval_accuracy = []\n","    eval_precision = []\n","    eval_recall = []\n","    eval_f1 = []\n","\n","    train_accuracy_values = []\n","    train_precision_values = []\n","    train_recall_values = []\n","    train_f1_values = []\n","\n","    for log in history:\n","        if 'loss' in log and 'eval_loss' not in log:\n","            train_loss.append((log['epoch'], log['loss']))\n","\n","        if 'eval_loss' in log:\n","            eval_loss.append((log['epoch'], log['eval_loss']))\n","            eval_accuracy.append((log['epoch'], log['eval_accuracy']))\n","            eval_precision.append((log['epoch'], log['eval_precision']))\n","            eval_recall.append((log['epoch'], log['eval_recall']))\n","            eval_f1.append((log['epoch'], log['eval_f1']))\n","\n","    for metrics in train_metrics_history:\n","        train_accuracy_values.append((metrics['epoch'], metrics['train_accuracy']))\n","        train_precision_values.append((metrics['epoch'], metrics['train_precision']))\n","        train_recall_values.append((metrics['epoch'], metrics['train_recall']))\n","        train_f1_values.append((metrics['epoch'], metrics['train_f1']))\n","\n","    train_epochs, train_loss_values = zip(*train_loss) if train_loss else ([], [])\n","    eval_epochs, eval_loss_values = zip(*eval_loss) if eval_loss else ([], [])\n","    _, eval_accuracy_values = zip(*eval_accuracy) if eval_accuracy else ([], [])\n","    _, eval_precision_values = zip(*eval_precision) if eval_precision else ([], [])\n","    _, eval_recall_values = zip(*eval_recall) if eval_recall else ([], [])\n","    _, eval_f1_values = zip(*eval_f1) if eval_f1 else ([], [])\n","\n","    train_epochs_accuracy, train_accuracy_values = zip(*train_accuracy_values) if train_accuracy_values else ([], [])\n","    train_epochs_precision, train_precision_values = zip(*train_precision_values) if train_precision_values else ([], [])\n","    train_epochs_recall, train_recall_values = zip(*train_recall_values) if train_recall_values else ([], [])\n","    train_epochs_f1, train_f1_values = zip(*train_f1_values) if train_f1_values else ([], [])\n","\n","    # Plot the metrics\n","    plt.figure(figsize=(18, 10))\n","\n","    plt.subplot(2, 2, 1)\n","    plt.plot(train_epochs, train_loss_values, label='Train Loss', marker='o')\n","    plt.plot(eval_epochs, eval_loss_values, label='Eval Loss', marker='o')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.title('Training and Evaluation Loss')\n","\n","    plt.subplot(2, 2, 2)\n","    plt.plot(eval_epochs, eval_accuracy_values, label='Eval Accuracy', marker='o')\n","    plt.plot(train_epochs_accuracy, train_accuracy_values, label='Train Accuracy', marker='o')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","    plt.title('Train and Evaluation Accuracy')\n","\n","    plt.subplot(2, 2, 3)\n","    plt.plot(eval_epochs, eval_precision_values, label='Eval Precision', marker='o')\n","    plt.plot(train_epochs_precision, train_precision_values, label='Train Precision', marker='o')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Precision')\n","    plt.legend()\n","    plt.title('Train and Evaluation Precision')\n","\n","    plt.subplot(2, 2, 4)\n","    plt.plot(eval_epochs, eval_recall_values, label='Eval Recall', marker='o')\n","    plt.plot(train_epochs_recall, train_recall_values, label='Train Recall', marker='o')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Recall')\n","    plt.legend()\n","    plt.title('Train and Evaluation Recall')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # Plot F1 scores separately\n","    plt.figure(figsize=(8, 5))\n","    plt.plot(eval_epochs, eval_f1_values, label='Eval F1 Score', marker='o')\n","    plt.plot(train_epochs_f1, train_f1_values, label='Train F1 Score', marker='o')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('F1 Score')\n","    plt.legend()\n","    plt.title('Train and Evaluation F1 Score')\n","    plt.show()\n"],"metadata":{"id":"GwV55TABDJkt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_metrics(history, train_metrics_history)"],"metadata":{"id":"KIacXV_9DKYD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_name = \"شهاب\"\n","predict_and_evaluate(model, tokenizer, new_name, label_encoder)"],"metadata":{"id":"FnSWAkRuDNlg"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1JhfIWLQIWThoVJQFp53laikzov5FefPC","timestamp":1723413809472},{"file_id":"1Eu1t9pcACtQetzKe9tVAIYTvfuRR8wWe","timestamp":1710396629153}],"toc_visible":true,"collapsed_sections":["CAwItQmWJzxa"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}